{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-익명성\n",
    "\n",
    "아래의 코드는 아래 Nuclearstar 원코드를 번역 및 주석처리\n",
    "- [Original Github Page](https://github.com/Nuclearstar/K-Anonymity)\n",
    "\n",
    "k-익명성 모델중 아래의 Mondrian 알고리즘을 구현한 모델임\n",
    "- [Mondrian - Multidimensional k-Anonymity](https://www.utdallas.edu/~muratk/courses/privacy08f_files/MultiDim.pdf)\n",
    "\n",
    "\n",
    "## k-익명성 구현\n",
    "\n",
    "- 데이터셋에서 klt 프라이버시 보호를 만족하는 최적의 데이터를 찾는 문제는 NP-hard 문제임\n",
    "- 근사 알고리즘을 통해 개인정보를 보호하는 문제를 해결하고자함\n",
    "\n",
    "해당 코드는 몬드리안 알고리즘을 구현 하였고 몬드리안 알고리즘은 모든 속성을 숫자형 or 범주형 변수로 변환 한 후에 각 속성에 대해 'span' 값을 계산하여 익명화 시도\n",
    "\n",
    "### 데이터 파티션\n",
    "\n",
    "알고리즘은 다음의 단계를 통하여서 k익명성을 만족하는 그룹으로 분할:\n",
    "\n",
    "1. 최종적으로 분할할 집합을 초기화 $P_{finished} = \\{\\}$.\n",
    "2. 분할 해야 할 전체 데이터 집합을 초기화 $P_{working} = \\{\\{1, 2,\\dots ,N\\}\\}$.\n",
    "4. 분할 되지 않은 데이터가 있을 때 각각의 분할 집합에 대하여 :\n",
    "  * 분할할 열에 대해 내림차순으로 정렬\n",
    "  * 정렬된 각 열에 대해서 아래의 과정을 반복\n",
    "      * 해당 열의 중앙값을 기준으로 데이터를 분할\n",
    "      * 분할된 결과가 k-익명성을 만족하는지 체크\n",
    "      * 만족하면 분할된 결과를 저장하고 반복문 종료\n",
    "  * 분할을 생성하지 못했을 경우, 원본 열을 $P_{finished}$에 반한\n",
    "5. 최종 분할된 집합$P_{finished}$을 반환\n",
    "\n",
    "### 데이터 익명화\n",
    "\n",
    "데이터를 분할한 후에 민감속성에 대해서 일반화를 통해서 준식별자가 k-익명성을 만족하도록 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas 패키지를 사용하여 데이터 프레임 작업을 할 수 있음\n",
    "# import (패키지이름) as (코드내에서 사용할 약어)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 열 이름 지정 (데이터셋에 열이름이 없음)\n",
    "names = (\n",
    "    'age',\n",
    "    'workclass', \n",
    "    'fnlwgt', # 중복되는 사용자를 해당 가중치를 이용하여 표현\n",
    "    'education',\n",
    "    'education-num',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'race',\n",
    "    'sex',\n",
    "    'capital-gain',\n",
    "    'capital-loss',\n",
    "    'hours-per-week',\n",
    "    'native-country',\n",
    "    'income',\n",
    ")\n",
    "\n",
    "# 범주형 데이터에 대해 미리 지정해줌\n",
    "#set 함수는 R에서 사용한 as.factor함수와 유사한 역할을 해줌\n",
    "categorical = set((\n",
    "    'workclass',\n",
    "    'education',\n",
    "    'marital-status',\n",
    "    'occupation',\n",
    "    'relationship',\n",
    "    'sex',\n",
    "    'native-country',\n",
    "    'race',\n",
    "    'income',\n",
    "))\n",
    "#데이터 입력 pandas패키지를 pd라는 약어로 사용하기로 정의 하였기 때문에 pd.read_csv를 통해서 데이터 입력\n",
    "#해당 코드와 같은 폴더 내에 데이터가 존재 하므로 데이터 이름만 입력(\"adult.all.txt\")\n",
    "#sep을 통해 데이터의 구분자를 지정 해당 데이터의 경우 쉼표로 데이터가 구분되기 때문에 \", \"\n",
    "#데이터의 첫 행에 열 이름이 있을 경우 header변수에 True 값을 주면 된다.\n",
    "df = pd.read_csv(\"adult.all.txt\", sep=\", \", header=None, names=names, index_col=False, engine='python');# We load the data using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupter 노트 북의 경우 변수명을 실행하면 print 하게 됨 (어떤 에디터의 경우에는 print 함수를 사용해줘야함)\n",
    "#pandas에서 제공하는 df.head() 를 통해 데이터의 상단 부분만 출력\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical 리스트에 있는 변수명에 대하여 astype('category') = 범주형 자료 로 변환\n",
    "#R 데이터 프레임과 동일하게 열 번호 혹은 df['열이름']을 통해서 데이터를 불러 올 수 있음\n",
    "for name in categorical:\n",
    "    df[name] = df[name].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**분할된 데이터 프레임의 모든 열에 대해 span을 계산하는 함수 구현** \n",
    "\n",
    "span\n",
    "\n",
    "숫자형 데이터 : 최대값과 최소값의 차이\n",
    "\n",
    "범주형 데이터 : 서로 구별데는 범주의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python의 경우 def 함수명 (입력 변수)를 통해서 함수를 정의 함\n",
    "\n",
    "def get_spans(df, partition, scale=None):\n",
    "    \"\"\"\n",
    "    입력 변수 설명\n",
    "            df: span을 계산할 데이터 프레임\n",
    "     partition: span을 계산할 파티션\n",
    "         scale: 스케일 값이 주어질 경우 각 열을 해당 스케일의 범위로 나눔\n",
    "    출력 변수 설명\n",
    "       returns: 파티션에 있는 모든 열의 span을 출력\n",
    "    \"\"\"\n",
    "    #spans 라는 해시 태이블 정의\n",
    "    spans = {}\n",
    "    #각 열에 대하여\n",
    "    for column in df.columns:\n",
    "        #범주형 자료일 경우\n",
    "        if column in categorical:\n",
    "            span = len(df[column][partition].unique())\n",
    "        #숫자혀 자료일 경우\n",
    "        else:\n",
    "            span = df[column][partition].max()-df[column][partition].min()\n",
    "        #sacle이 입력 되었을 경우\n",
    "        if scale is not None:\n",
    "            span = span/scale[column]\n",
    "        spans[column] = span\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df에 대하여 span 구하기\n",
    "full_spans = get_spans(df, df.index)\n",
    "full_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**데이터의 중앙 값을 기준으로 데이터를 분할하는 `split` 함수를 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df, partition, column):\n",
    "    \"\"\"\n",
    "    입력 변수 설명\n",
    "            df: split 할 데이터 프레임\n",
    "     partition: split 할 파티션\n",
    "        column: split 할 열 지정\n",
    "    출력 변수 설명\n",
    "       returns: split된 결과 값\n",
    "    \"\"\"\n",
    "    #해당 열의 해당 파티션 지정\n",
    "    dfp = df[column][partition]\n",
    "    #범주형 자료일 경우\n",
    "    if column in categorical:\n",
    "        #범주형 자료의 범주 추출 dfp.unique()\n",
    "        values = dfp.unique()\n",
    "        #범주를 반으로 나눔\n",
    "        lv = set(values[:len(values)//2])\n",
    "        rv = set(values[len(values)//2:])\n",
    "        return dfp.index[dfp.isin(lv)], dfp.index[dfp.isin(rv)]\n",
    "    #숫자형 자료일 경우\n",
    "    else:\n",
    "        #중앙 값\n",
    "        median = dfp.median()\n",
    "        dfl = dfp.index[dfp < median]\n",
    "        dfr = dfp.index[dfp >= median]\n",
    "        return (dfl, dfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 보조 함수를 사용하여 분할 알고리즘을 구현\n",
    "\n",
    "**k-익명성을 만족시키는 데이터 분할(일반화) 알고리즘 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k-익명성을 만족하는지 체크하는 함수\n",
    "def is_k_anonymous(df, partition, sensitive_column, k=3):\n",
    "    \"\"\"\n",
    "    입력 변수 설명\n",
    "                   df: 파티션의 k익명성을 확인할 데이터 프레임\n",
    "            partition: 데이터 프레임 내의 파티션\n",
    "     sensitive_column: 민감 속성\n",
    "                    k: k\n",
    "    출력 변수 설명\n",
    "    :returns         : k익명성을 만족하면 True 출력 만족하지 못하면 False를 출력\n",
    "    \"\"\"\n",
    "    if len(partition) < k:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#데이터 분할 함수\n",
    "def partition_dataset(df, feature_columns, sensitive_column, scale, is_valid):\n",
    "    \"\"\"\n",
    "    입력 변수 설명\n",
    "                   df: 파티션 할 데이터 프레임\n",
    "      feature_columns: 데이터를 분할(일반화) 하는데 사용되는 열 이름 목록(준식별자)\n",
    "     sensitive_column: 민감 속성 이름\n",
    "                scale: span 함수의 scale\n",
    "             is_valid: 데이터가 유효한 경우 True값을 출력\n",
    "    출력 변수 설명\n",
    "    :returns         : 파티션이 완료된 유효한 데이터 프레임\n",
    "    \"\"\"\n",
    "    \n",
    "    finished_partitions = []\n",
    "    partitions = [df.index]\n",
    "    while partitions:\n",
    "        partition = partitions.pop(0)\n",
    "        #각 열에 대해 span\n",
    "        spans = get_spans(df[feature_columns], partition, scale)\n",
    "        #정렬 후 split 함수를 적용하여 데이터 분할\n",
    "        for column, span in sorted(spans.items(), key=lambda x:-x[1]):\n",
    "            lp, rp = split(df, partition, column)\n",
    "            if not is_valid(df, lp, sensitive_column) or not is_valid(df, rp, sensitive_column):\n",
    "                continue\n",
    "            partitions.extend((lp, rp))\n",
    "            break\n",
    "        else:\n",
    "            finished_partitions.append(partition)\n",
    "    return finished_partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터를 진행 할 경우 알고리즘 수행시간이 오래 걸리기 때문에 간단한 예를 위해서 두개의 준 식별자와 하나의 민감정보만 선탁해여 결과를 확인 후 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply our partitioning method to two columns of our dataset, using \"income\" as the sensitive attribute\n",
    "feature_columns = ['age', 'education-num']\n",
    "sensitive_column = 'income'\n",
    "finished_partitions = partition_dataset(df, feature_columns, sensitive_column, full_spans, is_k_anonymous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the number of partitions that were created\n",
    "len(finished_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성된 파티션을 시각화, 핕션의 사각형 경계를 얻는 함수를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #matplotlib함수를 사용 하여 시각화 패키지내의 모듈만 import 하여 사용 할 수 있음\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indexes(df):\n",
    "    indexes = {}\n",
    "    for column in categorical:\n",
    "        values = sorted(df[column].unique())\n",
    "        indexes[column] = { x : y for x, y in zip(values, range(len(values)))}\n",
    "    return indexes\n",
    "\n",
    "def get_coords(df, column, partition, indexes, offset=0.1):\n",
    "    if column in categorical:\n",
    "        sv = df[column][partition].sort_values()\n",
    "        l, r = indexes[column][sv[sv.index[0]]], indexes[column][sv[sv.index[-1]]]+1.0\n",
    "    else:\n",
    "        sv = df[column][partition].sort_values()\n",
    "        next_value = sv[sv.index[-1]]\n",
    "        larger_values = df[df[column] > next_value][column]\n",
    "        if len(larger_values) > 0:\n",
    "            next_value = larger_values.min()\n",
    "        l = sv[sv.index[0]]\n",
    "        r = next_value\n",
    "    # we add some offset to make the partitions more easily visible\n",
    "    l -= offset\n",
    "    r += offset\n",
    "    return l, r\n",
    "\n",
    "def get_partition_rects(df, partitions, column_x, column_y, indexes, offsets=[0.1, 0.1]):\n",
    "    rects = []\n",
    "    for partition in partitions:\n",
    "        xl, xr = get_coords(df, column_x, partition, indexes, offset=offsets[0])\n",
    "        yl, yr = get_coords(df, column_y, partition, indexes, offset=offsets[1])\n",
    "        rects.append(((xl, yl),(xr, yr)))\n",
    "    return rects\n",
    "\n",
    "def get_bounds(df, column, indexes, offset=1.0):\n",
    "    if column in categorical:\n",
    "        return 0-offset, len(indexes[column])+offset\n",
    "    return df[column].min()-offset, df[column].max()+offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the bounding rects of all partitions that we created\n",
    "indexes = build_indexes(df)\n",
    "column_x, column_y = feature_columns[:2]\n",
    "rects = get_partition_rects(df, finished_partitions, column_x, column_y, indexes, offsets=[0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how our rects look like\n",
    "rects[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot the rects\n",
    "def plot_rects(df, ax, rects, column_x, column_y, edgecolor='black', facecolor='none'):\n",
    "    for (xl, yl),(xr, yr) in rects:\n",
    "        ax.add_patch(patches.Rectangle((xl,yl),xr-xl,yr-yl,linewidth=1,edgecolor=edgecolor,facecolor=facecolor, alpha=0.5))\n",
    "    ax.set_xlim(*get_bounds(df, column_x, indexes))\n",
    "    ax.set_ylim(*get_bounds(df, column_y, indexes))\n",
    "    ax.set_xlabel(column_x)\n",
    "    ax.set_ylabel(column_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(20,20))\n",
    "ax = pl.subplot(111)\n",
    "plot_rects(df, ax, rects, column_x, column_y, facecolor='r')\n",
    "pl.scatter(df[column_x], df[column_y])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-익명성을 만족하는 데이터 셋 생성\n",
    "\n",
    "민감정보와 일반화된 준식별자만을 포함하는 새로운 k-익명성을 만족하는 데이터 셋을 생성 하려고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#범주형 자료의 경우 ','문자열로 구분하여 데이터를 join하여 반환\n",
    "def agg_categorical_column(series):\n",
    "    return [','.join(set(series))]\n",
    "#숫자 변수의 경우 평균값을 반환\n",
    "def agg_numerical_column(series):\n",
    "    return [series.mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#비식별화 데이터셋 만들기\n",
    "def build_anonymized_dataset(df, partitions, feature_columns, sensitive_column, max_partitions=None):\n",
    "    #해쉬 테이블 사용\n",
    "    aggregations = {}\n",
    "    #각 변수에 대해\n",
    "    for column in feature_columns:\n",
    "        #범주형 자료의 경우 agg_categorical_column함수를 사용\n",
    "        if column in categorical:\n",
    "            aggregations[column] = agg_categorical_column\n",
    "        #숫자 변수의 경우 agg_numerical_column사용\n",
    "        else:\n",
    "            aggregations[column] = agg_numerical_column\n",
    "    rows = []\n",
    "    # 반복문 사용시 enumerate함수를 통해 loop가 실행 되었는지 확인 할 수 있음\n",
    "    for i, partition in enumerate(partitions):\n",
    "        #100으로 나눠서 나머지 1인 경우 몇번째 파티션 중인지 출력\n",
    "        if i % 100 == 1:\n",
    "            print(\"Finished {} partitions...\".format(i))\n",
    "        # 최대 반복수를 지정해주었을 경우 최대 반복수를 만족하면 알고리즘 종료\n",
    "        if max_partitions is not None and i > max_partitions:\n",
    "            break\n",
    "        grouped_columns = df.loc[partition].agg(aggregations, squeeze=False)\n",
    "        #민감열의 그룹별로 개수 count        \n",
    "        sensitive_counts = df.loc[partition].groupby(sensitive_column).agg({sensitive_column : 'count'})\n",
    "        values = grouped_columns.iloc[0].to_dict()\n",
    "        \n",
    "        for sensitive_value, count in sensitive_counts[sensitive_column].items():\n",
    "            if count == 0:\n",
    "                continue\n",
    "            values.update({\n",
    "                sensitive_column : sensitive_value,\n",
    "                'count' : count,\n",
    "\n",
    "            })\n",
    "            rows.append(values.copy())\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfn = build_anonymized_dataset(df, finished_partitions, feature_columns, sensitive_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 민감 속성 기준으로 데이터 정렬\n",
    "dfn.sort_values(feature_columns+[sensitive_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l-다양성 구현 (the naive way)\n",
    "\n",
    "k-익명성의 단점을 보완하기 위해 l-다양성 구현\n",
    "\n",
    "* 데이터의 유효성을 검증하는 `is_valid` 값을 수정하여 l-다양성을 만족하도록 함\n",
    "* `split` 함수를 수정하여 다양한 split을 생성 (가능 하다면)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**민감 정보 내의 파티션에 적어도 l개의 다른 민감한 속성 값이 존재하면 `True`, l-다양성을 만족하지 않으면 `False`를 반환하는 함수 작성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#해당 파티션내의 데이터의 다양성을 계산하는 함수\n",
    "def diversity(df, partition, column):\n",
    "    return len(df[column][partition].unique())\n",
    "\n",
    "#데이터의 다양성이 l개 이상인지 체크하는 함수\n",
    "def is_l_diverse(df, partition, sensitive_column, l=2):\n",
    "    \"\"\"\n",
    "    입력 변 수 설명\n",
    "    :param               df: The dataframe for which to check l-diversity\n",
    "    :param        partition: The partition of the dataframe on which to check l-diversity\n",
    "    :param sensitive_column: The name of the sensitive column\n",
    "    :param                l: The minimum required diversity of sensitive attribute values in the partition\n",
    "    출력 변수 설명\n",
    "                           : True or False\n",
    "    \"\"\"\n",
    "    \n",
    "    return diversity(df, partition, sensitive_column) >= l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#기존 분할 함수에 l-다양성 적용\n",
    "finished_l_diverse_partitions = partition_dataset(df, feature_columns, sensitive_column, full_spans, lambda *args: is_k_anonymous(*args) and is_l_diverse(*args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finished_l_diverse_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_x, column_y = feature_columns[:2]\n",
    "l_diverse_rects = get_partition_rects(df, finished_l_diverse_partitions, column_x, column_y, indexes, offsets=[0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(20,20))\n",
    "ax = pl.subplot(111)\n",
    "plot_rects(df, ax, l_diverse_rects, column_x, column_y, edgecolor='b', facecolor='b')\n",
    "plot_rects(df, ax, rects, column_x, column_y, facecolor='r')\n",
    "pl.scatter(df[column_x], df[column_y])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ㅣ다양성을 만족시키는 익명화 셋 생성\n",
    "dfl = build_anonymized_dataset(df, finished_l_diverse_partitions, feature_columns, sensitive_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 정렬\n",
    "dfl.sort_values([column_x, column_y, sensitive_column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-근접성 구현\n",
    "\n",
    "l-다양성 데이터의 경우 각각 민감 속성이 다양하게 분포한 반면에 준식별자의 경우 다양성이 떨어지게 분포하게 됨, 실제 데이터셋과 익명화된 데이터셋의 분포 차이에서 오는 개인정보 유출을 막기 위해 t-근접성 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**the Kolmogorov-Smirnov 거리를 통해 민감 속성과 전체 데이터 사이의 분포 거리를 계산 (the Kolmogorov-Smirnov 거리는 두 분포의 누적확률 분포의 거리를 나타내는 함수로 Kolmogorov-Smirnov test를 통해서 두 분포가 같은 분포인지 아닌지 테스트 할 수 있음\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 민감 속성에 대하여 전체 분포를 생성 할 수 있음\n",
    "global_freqs = {}\n",
    "total_count = float(len(df))\n",
    "group_counts = df.groupby(sensitive_column)[sensitive_column].agg('count')\n",
    "for value, count in group_counts.to_dict().items():\n",
    "    p = count/total_count\n",
    "    global_freqs[value] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_closeness(df, partition, column, global_freqs):\n",
    "    total_count = float(len(partition))\n",
    "    d_max = None\n",
    "    group_counts = df.loc[partition].groupby(column)[column].agg('count')\n",
    "    for value, count in group_counts.to_dict().items():\n",
    "        p = count/total_count\n",
    "        d = abs(p-global_freqs[value])\n",
    "        if d_max is None or d > d_max:\n",
    "            d_max = d\n",
    "    return d_max\n",
    "\n",
    "\n",
    "def is_t_close(df, partition, sensitive_column, global_freqs, p=0.2):\n",
    "    \"\"\"\n",
    "    입력 변수 설명\n",
    "    :param               df: The dataframe for which to check l-diversity\n",
    "    :param        partition: The partition of the dataframe on which to check l-diversity\n",
    "    :param sensitive_column: The name of the sensitive column\n",
    "    :param     global_freqs: The global frequencies of the sensitive attribute values\n",
    "    :param                p: The maximum allowed Kolmogorov-Smirnov distance\n",
    "    \"\"\"\n",
    "    if not sensitive_column in categorical:\n",
    "        raise ValueError(\"this method only works for categorical values\")\n",
    "    return t_closeness(df, partition, sensitive_column, global_freqs) <= p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비식별화에 적용\n",
    "finished_t_close_partitions = partition_dataset(df, feature_columns, sensitive_column, full_spans, lambda *args: is_k_anonymous(*args) and is_t_close(*args, global_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finished_t_close_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = build_anonymized_dataset(df, finished_t_close_partitions, feature_columns, sensitive_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how t-closeness fares\n",
    "dft.sort_values([column_x, column_y, sensitive_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_x, column_y = feature_columns[:2]\n",
    "t_close_rects = get_partition_rects(df, finished_t_close_partitions, column_x, column_y, indexes, offsets=[0.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(20,20))\n",
    "ax = pl.subplot(111)\n",
    "plot_rects(df, ax, t_close_rects, column_x, column_y, edgecolor='b', facecolor='b')\n",
    "pl.scatter(df[column_x], df[column_y])\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
